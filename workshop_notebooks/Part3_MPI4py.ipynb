{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: MPI for Python\n",
    "\n",
    "MPI is the __Message Passing Interface__ - a standard used for parallel programming involving communication between separate parallel processes each with their own separate memory allocation. MPI processes have to pass messages between themselves to invoke code execution and share data between with each other.  \n",
    "\n",
    "MPI is commonly used in distributed memory systems, computer systems composed of computer nodes, each with their own separate physical memory, such as high-performance computers or cluster machines. Equally, MPI will run just as well on your laptop computer and is not tied to any particular architecture. \n",
    "\n",
    "In this mini tutorial we introduce the `mpi4py` Python package, which provides an interface to the MPI libraries, similar to the corresponding C, C++, and Fortran MPI interfaces. \n",
    "\n",
    "If you are familiar with existing C/C++ MPI bindings, you will find picking up the syntax in the Python API very similar. However, don't worry if you are completely new to MPI - we only introduce the very basics of the standard here as an example of a Python parallel programming interface.\n",
    "\n",
    "## Following this tutorial\n",
    "\n",
    "Jupyter notebooks do not work particularly well with `mpi4py` (It is possible to set them up to work together, but that is beyond the scope of this tutorial.) We will demonstrate a few simple examples in this notebook, but to actually run the code it is easier to copy or type out the examples into a Python script and save it as a `.py` file. You would then execute the python script using the mpi launcher, usually called `mpirun` on most systems. For example to run a sample mpi python script over 4 processes, we would run the python script like so:\n",
    "\n",
    "```\n",
    "mpirun -np 4 python mpi_pi.py\n",
    "```\n",
    "\n",
    "To get started with any MPI script, we would add to our Python code the following import statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a simple test of `mpi4py` would print \"Hello world!\" or similar from each MPI process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm rank 0 from 1 running in total...\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "print(\"Hello! I'm rank {} from {} running in total...\".format(comm.rank, comm.size))\n",
    "\n",
    "# Wait for everyone to syncronize here:\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here?\n",
    "\n",
    "Firstly we set up a `comm` object, which is a communicator that lets us get information about and talk to the other processes. (You will see it in most MPI programs.)\n",
    "\n",
    "In the print function, we make a call to `comm.rank`, which gets the ID of the current process in the communicator (its 'rank') and then `comm.size` which gives is the total number of running processes in the communicator object.\n",
    "\n",
    "At the end we have to call `comm.Barrier` to synchronise the processes in the communicator.\n",
    "\n",
    "Unfortunately, if running this in a jupyter notebook, we will only likely see rank 0 and 1 total process, because of the way Python is running inside a single process. To see the effects of multiple MPI processes, we would ideally run the above code as a script from the terminal as outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting example\n",
    "\n",
    "Another example that demonstrates the interface with MPI is broadcasting a vector to `n` MPI processes. Broadcasting is a commonly used techniqe in MPI where a copy of some data is sent to every process (to be used to perform some further calculation on it by each process).\n",
    "\n",
    "In this example, we are going to take a 1D numpy array and _broadcast_ it from rank 0 to the other ranks (processes). \n",
    "\n",
    "***Note**: Perhaps confusingly MPI and NumPy both use the term _broadcasting_ to mean different things.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      " Running on 1 cores\n",
      "------------------------------------------------------------------------------\n",
      "[00] [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "print(\"-\"*78)\n",
    "print(\" Running on %d cores\" % comm.size)\n",
    "print(\"-\"*78)\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "# Prepare a vector of N=5 elements to be broadcasted...\n",
    "N = 5\n",
    "if comm.rank == 0:\n",
    "    A = np.arange(N, dtype=np.float64)    # rank 0 has the proper data\n",
    "else:\n",
    "    A = np.empty(N, dtype=np.float64)     # all other ranks just an empty array\n",
    "\n",
    "# Broadcast array A from rank 0 to everybody\n",
    "comm.Bcast( [A, MPI.DOUBLE] )\n",
    "# The list argument contains the array to be broadcast and the corresponding\n",
    "# MPI data type: MPI.DOUBLE\n",
    "\n",
    "# Everybody should now have the same...\n",
    "print(\"[%02d] %s\" % (comm.rank, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how if you are running this in a Jupyter notebook, you are only going to see `1 core` reported. This is because we are running the python code through a standard Python interpreter, rather than invoking it with a command like `mpirun`. To see the effects of this with multiple MPI processes running at once, you need to run the code above in a script called `mpi_broadcast.py` from the command line with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mpirun -np 4 python mpi_broadcast.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command now invokes the `python` interpreter through the `mpirun` executable, allowing the mpi4py python library to interact with the MPI API, and run on multiple cores on a multicore CPU.\n",
    "\n",
    "The output (from the terminal) should now look something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "------------------------------------------------------------------------------\n",
    " Running on 4 cores\n",
    "------------------------------------------------------------------------------\n",
    "[00] [0. 1. 2. 3. 4.]\n",
    "------------------------------------------------------------------------------\n",
    " Running on 4 cores\n",
    "------------------------------------------------------------------------------\n",
    "[01] [0. 1. 2. 3. 4.]\n",
    "------------------------------------------------------------------------------\n",
    " Running on 4 cores\n",
    "------------------------------------------------------------------------------\n",
    "[02] [0. 1. 2. 3. 4.]\n",
    "------------------------------------------------------------------------------\n",
    " Running on 4 cores\n",
    "------------------------------------------------------------------------------\n",
    "[03] [0. 1. 2. 3. 4.]\n",
    "```\n",
    "\n",
    "Note how now each process has received a copy of the same array from the broadcasting operation. In a real program, we might then proceed to mainpulate this array or perform calculations on it. \n",
    "\n",
    "## MPI Pi Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is approximately 3.1415927369231227, error is 0.0000000833333296\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "from math   import pi as PI\n",
    "from numpy  import array\n",
    "\n",
    "def get_n():\n",
    "    #prompt  = \"Enter the number of intervals: (0 quits) \"\n",
    "    try:\n",
    "        #n = int(input(prompt))\n",
    "        n = 1000\n",
    "        if n < 0: n = 0\n",
    "    except:\n",
    "        n = 0\n",
    "    return n\n",
    "\n",
    "def comp_pi(n, myrank=0, nprocs=1):\n",
    "    h = 1.0 / n\n",
    "    s = 0.0\n",
    "    for i in range(myrank + 1, n + 1, nprocs):\n",
    "        x = h * (i - 0.5)\n",
    "        s += 4.0 / (1.0 + x**2)\n",
    "    return s * h\n",
    "\n",
    "def prn_pi(pi, PI):\n",
    "    message = \"pi is approximately %.16f, error is %.16f\"\n",
    "    print  (message % (pi, abs(pi - PI)))\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "nprocs = comm.Get_size()\n",
    "myrank = comm.Get_rank()\n",
    "\n",
    "n    = array(0, dtype=int)\n",
    "pi   = array(0, dtype=float)\n",
    "mypi = array(0, dtype=float)\n",
    "\n",
    "#while True:\n",
    "def main():\n",
    "    if myrank == 0:\n",
    "        _n = get_n()\n",
    "        n.fill(_n)\n",
    "    comm.Bcast([n, MPI.INT], root=0)\n",
    "#    if n == 0:\n",
    "#        break\n",
    "    _mypi = comp_pi(n, myrank, nprocs)\n",
    "    mypi.fill(_mypi)\n",
    "    comm.Reduce([mypi, MPI.DOUBLE], [pi, MPI.DOUBLE],\n",
    "                op=MPI.SUM, root=0)\n",
    "    if myrank == 0:\n",
    "        prn_pi(pi, PI)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and taking it further\n",
    "\n",
    "**mpi4py** is a Python interface to the MPI library for message-passing parallel programming. It provides an interface to the powerful Message-Passing Interface standard, a parallel programming standard commonly used in distribute memory parallel programming.\n",
    "\n",
    "In _Part 1: Multiprocessing_, you may recall we discussed the `multiprocessing` module, another pytohn module that creates separate processes and can be used to distribute parallel tasks. Multiprocessing is limited to creating OS-level processes on a shared-memory computing environment, and (to my knowledge) is not easy to apply across multi-node cluster computers, something which MPI (and by extension `mpi4py`) was designed specifically to do.\n",
    "\n",
    "So if your problem size requires the resources of larger compute clusters, `mpi4py` may be the more appropriate choice, though the learning curve is inevitably steeper due to requiring knowledge of the MPI standard to get the most out of the Python implementation.\n",
    "\n",
    "### Useful resources for taking it further\n",
    "\n",
    "Introductory MPI texts and tutorials would be just as useful for an overview of MPI in greater depth. The current documentation for mpi4py, although good, does tend to assume some knowledge of the concepts behind message passing and distributed memory programming. Therefore, starting with a basic MPI tutorial may be a good first move. E.g.: http://mpitutorial.com/tutorials/ is a nice set of introductory tutorials.\n",
    "\n",
    "MPI for Python documentation: https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
    "\n",
    "Excellent Python MPI tutorial (goes into more depth) https://nyu-cds.github.io/python-mpi/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
